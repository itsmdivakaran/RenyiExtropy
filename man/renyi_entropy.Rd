% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/renyi_entropy.R
\name{renyi_entropy}
\alias{renyi_entropy}
\title{Rényi Entropy}
\usage{
renyi_entropy(p, q)
}
\arguments{
\item{p}{Numeric probability vector \eqn{\mathbf{p} = (p_1, \ldots, p_n)},
where each element is in \eqn{[0, 1]} and \eqn{\sum_{i=1}^n p_i = 1}.}

\item{q}{Numeric scalar. The Rényi parameter (must satisfy \eqn{q > 0} and \eqn{q \neq 1}).}
}
\value{
A numeric scalar giving the Rényi entropy value.
}
\description{
Computes the Rényi entropy for a discrete probability distribution and order parameter \eqn{q}.
}
\details{
The Rényi entropy is defined as:
\deqn{
  H_q(\mathbf{p}) = \frac{1}{1 - q} \log\left( \sum_{i=1}^n p_i^q \right)
}
For \eqn{q \to 1}, this reduces to the Shannon entropy:
\deqn{
  H(\mathbf{p}) = -\sum_{i=1}^n p_i \log p_i
}

The function automatically detects if \eqn{q} is numerically close to 1 and
returns the Shannon entropy in that case.
}
\examples{
# Example probability vector
p <- c(0.2, 0.5, 0.3)

# Compute Rényi entropy for q = 2
renyi_entropy(p, 2)

# Compute for q = 0.5
renyi_entropy(p, 0.5)

# Shannon entropy limit (q near 1)
renyi_entropy(p, 1.000001)

}
\seealso{
\code{\link{shannon_entropy}}, \code{\link{renyi_extropy}}, \code{\link{extropy}}
}
