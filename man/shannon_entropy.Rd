% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/shannon_entropy.R
\name{shannon_entropy}
\alias{shannon_entropy}
\title{Shannon Entropy}
\usage{
shannon_entropy(p)
}
\arguments{
\item{p}{Numeric probability vector \eqn{\mathbf{p} = (p_1, \ldots, p_n)},
where each element is in \eqn{[0, 1]} and \eqn{\sum_{i=1}^n p_i = 1}.}
}
\value{
A numeric scalar giving the Shannon entropy.
}
\description{
Computes the Shannon entropy for a discrete probability distribution.
}
\details{
The Shannon entropy is defined as:
\deqn{
  H(\mathbf{p}) = - \sum_{i=1}^n p_i \log(p_i)
}
with the convention that terms where \eqn{p_i = 0} contribute 0.
This is the limit of the RÃ©nyi entropy as \eqn{q \to 1}.
}
\examples{
p <- c(0.2, 0.5, 0.3)

# Compute Shannon entropy
shannon_entropy(p)

}
\seealso{
\code{\link{renyi_entropy}}, \code{\link{extropy}}, \code{\link{renyi_extropy}}
}
