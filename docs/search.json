[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Mahesh Divakaran Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Divakaran Mahesh. Author, maintainer. G Rajesh. Author. Sreekumar Jayalekshmi. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahesh D, Rajesh G, Jayalekshmi S (2025). RenyiExtropy: Entropy Extropy Measures Probability Distributions. R package version 0.3.0.","code":"@Manual{,   title = {RenyiExtropy: Entropy and Extropy Measures for Probability Distributions},   author = {Divakaran Mahesh and G Rajesh and Sreekumar Jayalekshmi},   year = {2025},   note = {R package version 0.3.0}, }"},{"path":"/index.html","id":"renyiextropy","dir":"","previous_headings":"","what":"Entropy and Extropy Measures for Probability Distributions","title":"Entropy and Extropy Measures for Probability Distributions","text":"RenyiExtropy package provides collection functions compute entropy extropy measures discrete probability distributions, including: Shannon Entropy Shannon Extropy Rényi Entropy Rényi Extropy (different q-orders) Tsallis Entropy Normalized Entropy measures useful information theory, statistics, signal processing, bioinformatics, machine learning quantifying uncertainty diversity datasets.","code":""},{"path":"/index.html","id":"installation--examples","dir":"","previous_headings":"","what":"Installation & Examples","title":"Entropy and Extropy Measures for Probability Distributions","text":"Entropy Extropy Measures Example Probability Vector","code":"# ------------------------------ # Installation (development version from GitHub) # ------------------------------ # install.packages(\"pak\") # pak::pak(\"itsmdivakaran/Renyi-Extropy\")   # OR  # Installation (development version from GitHub) install.packages(\"remotes\") ## Installing package into '/private/var/folders/k_/b3s11b6s6snf80p7kbq8mwl40000gn/T/RtmpwF0vCj/temp_libpath60f713849903' ## (as 'lib' is unspecified)  ##  ## The downloaded binary packages are in ##  /var/folders/k_/b3s11b6s6snf80p7kbq8mwl40000gn/T//RtmpFXUzg7/downloaded_packages remotes::install_github(\"itsmdivakaran/RenyiExtropy\") ## Using GitHub PAT from the git credential store.  ## Downloading GitHub repo itsmdivakaran/RenyiExtropy@HEAD  ## ── R CMD build ───────────────────────────────────────────────────────────────── ## * checking for file ‘/private/var/folders/k_/b3s11b6s6snf80p7kbq8mwl40000gn/T/RtmpFXUzg7/remotes62f84b8cebc8/itsmdivakaran-RenyiExtropy-69926c9/DESCRIPTION’ ... OK ## * preparing ‘RenyiExtropy’: ## * checking DESCRIPTION meta-information ... OK ## * checking for LF line-endings in source and make files and shell scripts ## * checking for empty or unneeded directories ## * building ‘RenyiExtropy_0.1.0.tar.gz’  ## Installing package into '/private/var/folders/k_/b3s11b6s6snf80p7kbq8mwl40000gn/T/RtmpwF0vCj/temp_libpath60f713849903' ## (as 'lib' is unspecified) # ------------------------------ # Load the package # ------------------------------ library(RenyiExtropy)  # ------------------------------ # Example probability vector # ------------------------------ p <- c(0.2, 0.3, 0.5)  # ------------------------------ # Demonstration of all functions # ------------------------------ results <- list(   Package_Version         = as.character(utils::packageVersion(\"RenyiExtropy\")),   Shannon_Entropy         = shannon_entropy(p),   Shannon_Extropy         = shannon_extropy(p),   Renyi_Entropy_q2         = renyi_entropy(p, q = 2),   Renyi_Extropy_q2         = renyi_extropy(p, q = 2),   Tsallis_Entropy_q1.5     = tsallis_entropy(p, q = 1.5),   Normalized_Entropy       = normalized_entropy(p) )  # Print results as a neat data frame for readability knitr::kable(   data.frame(     Measure = names(results),     Value   = unlist(results)   ),   caption = \"Entropy and Extropy Measures for Example Probability Vector\" )"},{"path":"/reference/conditional_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Conditional Entropy — conditional_entropy","title":"Calculate Conditional Entropy — conditional_entropy","text":"Calculate Conditional Entropy","code":""},{"path":"/reference/conditional_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Conditional Entropy — conditional_entropy","text":"","code":"conditional_entropy(joint_matrix)"},{"path":"/reference/conditional_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Conditional Entropy — conditional_entropy","text":"joint_matrix Matrix joint probabilities. Rows = X outcomes, Cols = Y outcomes.","code":""},{"path":"/reference/conditional_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Conditional Entropy — conditional_entropy","text":"Numeric value. Conditional entropy H(Y|X).","code":""},{"path":"/reference/conditional_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Conditional Entropy — conditional_entropy","text":"H(Y|X) = H(X,Y) - H(X)","code":""},{"path":"/reference/conditional_renyi_extropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Rényi Extropy — conditional_renyi_extropy","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"Computes conditional Rényi extropy \\(J_q(Y \\mid X)\\) two discrete random variables, given joint probability matrix.","code":""},{"path":"/reference/conditional_renyi_extropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"","code":"conditional_renyi_extropy(Pxy, q)"},{"path":"/reference/conditional_renyi_extropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"Pxy numeric matrix representing joint probability distribution \\(P_{ij}\\) (rows correspond values \\(X\\), columns correspond values \\(Y\\)). entries must non-negative sum 1. q numeric scalar specifying Rényi parameter. Must satisfy \\(q > 0\\) \\(q \\neq 1\\).","code":""},{"path":"/reference/conditional_renyi_extropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"numeric scalar giving conditional Rényi extropy.","code":""},{"path":"/reference/conditional_renyi_extropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"conditional Rényi extropy defined : $$   J_q(Y \\mid X) = J_q(X, Y) - J_q(X) $$ \\(J_q(\\cdot)\\) denotes Rényi extropy. \\(q \\1\\), converges conditional Shannon extropy.","code":""},{"path":[]},{"path":"/reference/conditional_renyi_extropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Rényi Extropy — conditional_renyi_extropy","text":"","code":"Pxy <- matrix(c(0.2, 0.3, 0.1, 0.4), nrow = 2, byrow = TRUE) conditional_renyi_extropy(Pxy, 2) #> [1] 0.1039623"},{"path":"/reference/cross_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cross-Entropy — cross_entropy","title":"Calculate Cross-Entropy — cross_entropy","text":"Calculate Cross-Entropy","code":""},{"path":"/reference/cross_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cross-Entropy — cross_entropy","text":"","code":"cross_entropy(p, q)"},{"path":"/reference/cross_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cross-Entropy — cross_entropy","text":"p Numeric vector. True probability distribution. q Numeric vector. Estimated probability distribution. length p.","code":""},{"path":"/reference/cross_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cross-Entropy — cross_entropy","text":"Numeric value. Cross-entropy H(P, Q).","code":""},{"path":"/reference/cross_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Cross-Entropy — cross_entropy","text":"Cross-entropy: \\(H(P,Q) = -\\sum p_i \\log q_i\\).","code":""},{"path":"/reference/extropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Classical Extropy — extropy","title":"Classical Extropy — extropy","text":"Computes classical extropy discrete probability distribution.","code":""},{"path":"/reference/extropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classical Extropy — extropy","text":"","code":"extropy(p)"},{"path":"/reference/extropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classical Extropy — extropy","text":"p numeric probability vector \\(\\mathbf{p} = (p_1, \\ldots, p_n)\\), element \\([0, 1]\\) \\(\\sum_{=1}^n p_i = 1\\).","code":""},{"path":"/reference/extropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classical Extropy — extropy","text":"numeric scalar giving classical extropy.","code":""},{"path":"/reference/extropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classical Extropy — extropy","text":"classical extropy defined : $$   J(\\mathbf{p}) = -\\sum_{=1}^n (1 - p_i) \\log(1 - p_i) $$ Terms \\(1 - p_i = 0\\) omitted avoid \\(\\log 0\\).","code":""},{"path":[]},{"path":"/reference/extropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classical Extropy — extropy","text":"","code":"p <- c(0.2, 0.5, 0.3) extropy(p) #> [1] 0.7747609  extropy(rep(0.25, 4)) #> [1] 0.8630462"},{"path":"/reference/joint_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Joint Entropy — joint_entropy","title":"Calculate Joint Entropy — joint_entropy","text":"Calculate Joint Entropy","code":""},{"path":"/reference/joint_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Joint Entropy — joint_entropy","text":"","code":"joint_entropy(joint_matrix)"},{"path":"/reference/joint_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Joint Entropy — joint_entropy","text":"joint_matrix Matrix joint probabilities. Rows = X outcomes, Cols = Y outcomes.","code":""},{"path":"/reference/joint_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Joint Entropy — joint_entropy","text":"Numeric value. Joint entropy H(X,Y).","code":""},{"path":"/reference/js_divergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Jensen-Shannon Divergence — js_divergence","title":"Calculate Jensen-Shannon Divergence — js_divergence","text":"Calculate Jensen-Shannon Divergence","code":""},{"path":"/reference/js_divergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Jensen-Shannon Divergence — js_divergence","text":"","code":"js_divergence(p, q)"},{"path":"/reference/js_divergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Jensen-Shannon Divergence — js_divergence","text":"p Numeric vector. First probability distribution. q Numeric vector. Second probability distribution. length p.","code":""},{"path":"/reference/js_divergence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Jensen-Shannon Divergence — js_divergence","text":"Numeric value. Jensen-Shannon divergence.","code":""},{"path":"/reference/js_divergence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Jensen-Shannon Divergence — js_divergence","text":"$$JSD(P, Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$ \\(M = (P+Q)/2\\).","code":""},{"path":"/reference/kl_divergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate KL Divergence — kl_divergence","title":"Calculate KL Divergence — kl_divergence","text":"Calculate KL Divergence","code":""},{"path":"/reference/kl_divergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate KL Divergence — kl_divergence","text":"","code":"kl_divergence(p, q)"},{"path":"/reference/kl_divergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate KL Divergence — kl_divergence","text":"p Numeric vector. True probability distribution. q Numeric vector. Estimated probability distribution. length p.","code":""},{"path":"/reference/kl_divergence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate KL Divergence — kl_divergence","text":"Numeric value. KL divergence D_KL(P || Q).","code":""},{"path":"/reference/kl_divergence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate KL Divergence — kl_divergence","text":"$$D_{KL}(P||Q) = \\sum p_i \\log (p_i / q_i)$$","code":""},{"path":"/reference/max_renyi_extropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"Computes maximum Rényi extropy, occurs uniform discrete distribution. maximum depends number outcomes \\(n\\) independent Rényi parameter \\(q\\).","code":""},{"path":"/reference/max_renyi_extropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"","code":"max_renyi_extropy(n)"},{"path":"/reference/max_renyi_extropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"n integer scalar giving number outcomes discrete distribution. Must satisfy \\(n \\ge 2\\).","code":""},{"path":"/reference/max_renyi_extropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"numeric scalar giving maximum Rényi extropy.","code":""},{"path":"/reference/max_renyi_extropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"uniform distribution \\(p_i = 1/n\\), maximum Rényi extropy : $$   \\max J_q(P) = (n - 1) \\log\\!\\left(\\frac{n}{n - 1}\\right) $$ holds \\(q > 0\\), \\(q \\neq 1\\).","code":""},{"path":[]},{"path":"/reference/max_renyi_extropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum Rényi Extropy for a Uniform Distribution — max_renyi_extropy","text":"","code":"# For 3 equally likely outcomes max_renyi_extropy(3) #> [1] 0.8109302  # For a uniform distribution with 10 outcomes max_renyi_extropy(10) #> [1] 0.9482446"},{"path":"/reference/normalized_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Normalized Entropy — normalized_entropy","title":"Calculate Normalized Entropy — normalized_entropy","text":"Calculate Normalized Entropy","code":""},{"path":"/reference/normalized_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Normalized Entropy — normalized_entropy","text":"","code":"normalized_entropy(p)"},{"path":"/reference/normalized_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Normalized Entropy — normalized_entropy","text":"p Numeric vector. Probability vector.","code":""},{"path":"/reference/normalized_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Normalized Entropy — normalized_entropy","text":"Numeric value. Normalized Shannon entropy \\([0, 1]\\).","code":""},{"path":"/reference/renyi_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Rényi Entropy — renyi_entropy","title":"Rényi Entropy — renyi_entropy","text":"Computes Rényi entropy discrete probability distribution order parameter \\(q\\).","code":""},{"path":"/reference/renyi_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rényi Entropy — renyi_entropy","text":"","code":"renyi_entropy(p, q)"},{"path":"/reference/renyi_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rényi Entropy — renyi_entropy","text":"p Numeric probability vector \\(\\mathbf{p} = (p_1, \\ldots, p_n)\\), element \\([0, 1]\\) \\(\\sum_{=1}^n p_i = 1\\). q Numeric scalar. Rényi parameter (must satisfy \\(q > 0\\) \\(q \\neq 1\\)).","code":""},{"path":"/reference/renyi_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rényi Entropy — renyi_entropy","text":"numeric scalar giving Rényi entropy value.","code":""},{"path":"/reference/renyi_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rényi Entropy — renyi_entropy","text":"Rényi entropy defined : $$   H_q(\\mathbf{p}) = \\frac{1}{1 - q} \\log\\left( \\sum_{=1}^n p_i^q \\right) $$ \\(q \\1\\), reduces Shannon entropy: $$   H(\\mathbf{p}) = -\\sum_{=1}^n p_i \\log p_i $$ function automatically detects \\(q\\) numerically close 1 returns Shannon entropy case.","code":""},{"path":[]},{"path":"/reference/renyi_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rényi Entropy — renyi_entropy","text":"","code":"# Example probability vector p <- c(0.2, 0.5, 0.3)  # Compute Rényi entropy for q = 2 renyi_entropy(p, 2) #> [1] 0.967584  # Compute for q = 0.5 renyi_entropy(p, 0.5) #> [1] 1.063659  # Shannon entropy limit (q near 1) renyi_entropy(p, 1.000001) #> [1] 1.029653"},{"path":"/reference/renyi_extropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Rényi Extropy — renyi_extropy","title":"Calculate the Rényi Extropy — renyi_extropy","text":"Computes Rényi extropy given probability vector Rényi parameter.","code":""},{"path":"/reference/renyi_extropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Rényi Extropy — renyi_extropy","text":"","code":"renyi_extropy(p, q)"},{"path":"/reference/renyi_extropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Rényi Extropy — renyi_extropy","text":"p Numeric vector. Probability vector. Elements must [0, 1] sum 1. q Numeric scalar. Rényi parameter. Real number, equal 1.","code":""},{"path":"/reference/renyi_extropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Rényi Extropy — renyi_extropy","text":"Numeric value. calculated Rényi extropy.","code":""},{"path":"/reference/renyi_extropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Rényi Extropy — renyi_extropy","text":"Rényi extropy order \\(q\\) defined : $$J_q(P) = \\frac{-(n-1)\\log(n-1) + (n-1)\\log\\left( \\sum_{=1}^n (1 - p_i)^q \\right)}{1 - q}$$ \\(n\\) number elements \\(P\\). \\(q \\1\\), function returns Shannon extropy: $$J(P) = -\\sum_{=1}^n (1 - p_i) \\log(1 - p_i)$$. \\(n = 2\\), Rényi extropy equals Rényi entropy. Throws error p valid probability vector q real number.","code":""},{"path":[]},{"path":"/reference/renyi_extropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Rényi Extropy — renyi_extropy","text":"","code":"p <- c(0.2, 0.5, 0.3) renyi_extropy(p, 2) #> [1] 0.7421274 renyi_extropy(p, 1.1) #> [1] 0.7713296"},{"path":"/reference/shannon_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Shannon Entropy — shannon_entropy","title":"Shannon Entropy — shannon_entropy","text":"Computes Shannon entropy discrete probability distribution.","code":""},{"path":"/reference/shannon_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shannon Entropy — shannon_entropy","text":"","code":"shannon_entropy(p)"},{"path":"/reference/shannon_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shannon Entropy — shannon_entropy","text":"p Numeric probability vector \\(\\mathbf{p} = (p_1, \\ldots, p_n)\\), element \\([0, 1]\\) \\(\\sum_{=1}^n p_i = 1\\).","code":""},{"path":"/reference/shannon_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shannon Entropy — shannon_entropy","text":"numeric scalar giving Shannon entropy.","code":""},{"path":"/reference/shannon_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shannon Entropy — shannon_entropy","text":"Shannon entropy defined : $$   H(\\mathbf{p}) = - \\sum_{=1}^n p_i \\log(p_i) $$ convention terms \\(p_i = 0\\) contribute 0. limit Rényi entropy \\(q \\1\\).","code":""},{"path":[]},{"path":"/reference/shannon_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shannon Entropy — shannon_entropy","text":"","code":"p <- c(0.2, 0.5, 0.3)  # Compute Shannon entropy shannon_entropy(p) #> [1] 1.029653"},{"path":"/reference/shannon_extropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Shannon Extropy — shannon_extropy","title":"Calculate the Shannon Extropy — shannon_extropy","text":"Computes Shannon extropy given probability vector.","code":""},{"path":"/reference/shannon_extropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Shannon Extropy — shannon_extropy","text":"","code":"shannon_extropy(p)"},{"path":"/reference/shannon_extropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Shannon Extropy — shannon_extropy","text":"p Numeric vector. Probability vector. Elements must \\([0, 1]\\) sum 1.","code":""},{"path":"/reference/shannon_extropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Shannon Extropy — shannon_extropy","text":"Numeric value. Shannon extropy provided vector.","code":""},{"path":"/reference/shannon_extropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Shannon Extropy — shannon_extropy","text":"Shannon extropy defined \\(J(P) = -\\sum_{=1}^n (1 - p_i) \\log(1 - p_i)\\). probability \\(p_i = 1\\), term treated 0.","code":""},{"path":[]},{"path":"/reference/shannon_extropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Shannon Extropy — shannon_extropy","text":"","code":"p <- c(0.2, 0.5, 0.3) shannon_extropy(p) #> [1] 0.7747609"},{"path":"/reference/tsallis_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Tsallis Entropy — tsallis_entropy","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"Computes Tsallis entropy given probability vector order \\(q\\).","code":""},{"path":"/reference/tsallis_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"","code":"tsallis_entropy(p, q)"},{"path":"/reference/tsallis_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"p Numeric vector. Probability vector. Elements must \\(0\\) \\(1\\) (inclusive) sum \\(1\\). q Numeric scalar. Tsallis parameter (\\(q > 0\\), \\(q \\neq 1\\)).","code":""},{"path":"/reference/tsallis_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"Numeric value. Tsallis entropy.","code":""},{"path":"/reference/tsallis_entropy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"Tsallis entropy defined : $$   H_q^{(T)}(P) = \\frac{1 - \\sum_i p_i^q}{q - 1} $$ \\(q \\1\\), reduces Shannon entropy.","code":""},{"path":[]},{"path":"/reference/tsallis_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Tsallis Entropy — tsallis_entropy","text":"","code":"p <- c(0.2, 0.5, 0.3) tsallis_entropy(p, 2) #> [1] 0.62"},{"path":[]},{"path":[]},{"path":"/news/index.html","id":"renyiextropy-010","dir":"Changelog","previous_headings":"","what":"RenyiExtropy 0.1.0","title":"RenyiExtropy 0.1.0","text":"Initial CRAN submission.","code":""}]
